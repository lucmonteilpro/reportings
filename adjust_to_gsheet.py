#!/usr/bin/env python3
"""
ADJUST ‚Üí GOOGLE SHEETS PIPELINE
Script standalone pour Sharper Media

Auteur: Refactoris√© pour reprise en main
Date: Novembre 2025
"""

import pandas as pd
import requests
import io
from datetime import datetime, timedelta
from google.oauth2.credentials import Credentials
from google_auth_oauthlib.flow import InstalledAppFlow
from google.auth.transport.requests import Request
from googleapiclient.discovery import build
import gspread
from gspread_dataframe import set_with_dataframe
import os
import pickle
import json

# =============================================================================
# CONFIGURATION
# =============================================================================

ADJUST_API_TOKEN = "KmP1b4iXsW6YSWJxN43g"  # ‚úÖ Token SA BFORBANK qui fonctionne

# Scopes pour Google Sheets
SCOPES = ['https://www.googleapis.com/auth/spreadsheets']

# Configuration SA BFORBANK / WEBANK iOS
BFORBANK_CONFIG = {
    "client": "SA BFORBANK Webank iOS",
    "app_token": "30kmesrwq3nk",  # ‚úÖ App Token Webank iOS
    "adjust_account_id": "29151",  # ‚úÖ Account ID SA BFORBANK - OBLIGATOIRE
    "sheet_id": "1ytoAiVBYn2QkqbiAAVnDicJCbjQLBM2aRZiTPo-dH8k",  # ‚úÖ Google Sheet
    "sheet_name": "raw_ios",  # ‚úÖ Nom de l'onglet
    "start_date": "2025-01-01",
    "custom_cpi": {},  # Pas de custom CPI - pas de cost disponible
    "agg_columns": [
        "Day (date)",
        "Country",
        "Network (attribution)",
        "Campaign (attribution)",
        "Adgroup (attribution)",
        "Creative (attribution)"
    ],
    "group_by_most_spending_campaign": False,
    "compute_ctr": False
}

# Configuration LALALAB Client Report iOS
LALALAB_IOS_CONFIG = {
    "client": "Lalalab Client Report ios",
    "app_token": "vmu6fbf5yprt",  # ‚úÖ App Token Lalalab iOS
    "adjust_account_id": "259",  # ‚úÖ Account ID LALALAB - OBLIGATOIRE
    "sheet_id": "1slh8klvy5KfgUGxJz7yLJ5YKZmRPBMe59ViqsGEOU_Q",  # ‚úÖ Nouveau Google Sheet
    "sheet_name": "raw_ios",  # ‚úÖ Onglet iOS
    "start_date": "2025-01-01",
    "custom_cpi": {
        "France": 7.0,
        "Germany": 5.0
    },
    "agg_columns": [
        "App",
        "Month (date)",
        "Week (date)",
        "Day (date)",
        "Network (attribution)",
        "Country",
        "Campaign (attribution)",
        "Adgroup (attribution)",
        "Creative (attribution)"
    ],
    "repush_all": True,  # ‚ö†Ô∏è IMPORTANT: Repush tout √† cause des d7/d30 qui changent
    "group_by_most_spending_campaign": False,
    "compute_ctr": False
}


# =============================================================================
# FONCTIONS GOOGLE SHEETS AUTH
# =============================================================================

def get_google_creds():
    """
    Authentification Google.
    Essaie d'abord le Service Account, sinon OAuth.
    """
    creds = None
    
    # Option 1: Service Account (recommand√© - c'est ce que ton tech utilise)
    service_account_file = 'service_account.json'
    if os.path.exists(service_account_file):
        from google.oauth2.service_account import Credentials as ServiceCredentials
        creds = ServiceCredentials.from_service_account_file(
            service_account_file,
            scopes=SCOPES
        )
        print("‚úÖ Auth via Service Account")
        return creds
    
    # Option 2: OAuth (fallback)
    token_file = 'token.pickle'
    
    # V√©rifie si on a d√©j√† des credentials sauvegard√©s
    if os.path.exists(token_file):
        with open(token_file, 'rb') as token:
            creds = pickle.load(token)
    
    # Si pas de credentials valides, on fait l'auth
    if not creds or not creds.valid:
        if creds and creds.expired and creds.refresh_token:
            creds.refresh(Request())
        else:
            # Tu dois avoir un fichier credentials.json (voir README)
            if not os.path.exists('credentials.json'):
                print("‚ùå ERREUR: Aucun fichier d'authentification trouv√©!")
                print("   Place soit 'service_account.json' (recommand√©)")
                print("   soit 'credentials.json' (OAuth) dans ce dossier.")
                print("   Voir le README pour plus de d√©tails.")
                return None
            
            flow = InstalledAppFlow.from_client_secrets_file('credentials.json', SCOPES)
            creds = flow.run_local_server(port=0)
        
        # Sauvegarde pour la prochaine fois
        with open(token_file, 'wb') as token:
            pickle.dump(creds, token)
    
    print("‚úÖ Auth via OAuth")
    return creds


def get_gspread_client():
    """Retourne un client gspread authentifi√©."""
    creds = get_google_creds()
    if creds is None:
        return None
    return gspread.authorize(creds)


# =============================================================================
# FONCTIONS ADJUST
# =============================================================================

def pull_from_adjust(
    app_token: str,
    begin_date: str,
    end_date: str,
    adjust_account_id: str = None,
    dimensions: str = "day,country,network,campaign,creative,adgroup",
    metrics: str = None,
    include_revenue: bool = False,
    events: list = None  # ‚úÖ AJOUT√â : Liste d'√©v√©nements (ex: ['first_purchase'])
) -> pd.DataFrame:
    """
    Pull les donn√©es depuis l'API Adjust.
    
    Args:
        app_token: Token de l'app Adjust (ex: "30kmesrwq3nk")
        begin_date: Date de d√©but (format YYYY-MM-DD)
        end_date: Date de fin (format YYYY-MM-DD)
        adjust_account_id: ID du compte Adjust (REQUIS pour certains comptes)
        dimensions: Dimensions √† r√©cup√©rer
        metrics: M√©triques √† r√©cup√©rer (si None, utilise les m√©triques par d√©faut)
        include_revenue: Si True, ajoute les m√©triques de revenue
        events: Liste d'√©v√©nements √† r√©cup√©rer (ex: ['first_purchase'])
    
    Returns:
        DataFrame avec les donn√©es Adjust
    """
    print(f"üì• Pull Adjust: {begin_date} ‚Üí {end_date}")
    
    # M√©triques par d√©faut selon le client
    if metrics is None:
        if include_revenue:
            metrics = "installs,clicks,impressions,revenue,all_revenue_total_d0,all_revenue_total_d7,all_revenue_total_d30"
        else:
            metrics = "installs,clicks,impressions"
    
    # ‚úÖ Ajouter les √©v√©nements aux m√©triques (SANS suffixes d0/d7/d30)
    if events:
        # On ajoute juste les √©v√©nements tels quels
        metrics = metrics + "," + ",".join(events)
        print(f"   üìä √âv√©nements ajout√©s: {', '.join(events)}")
    
    params = {
        "date_period": f"{begin_date}:{end_date}",
        "dimensions": dimensions,
        "metrics": metrics,
        "readable_names": True,
        "utc_offset": "+01:00",  # Paris timezone (UTC+1)
        "attribution_source": "first",  # ‚úÖ CORRIG√â : utilise First attribution (pas Dynamic)
        "attribution_type": "all",  # Type d'attribution (all/click/impression)
        "currency": "EUR",
        "app_token__in": app_token
    }
    
    # CRITIQUE : Ajouter l'account ID si fourni
    if adjust_account_id:
        params['adjust_account_id'] = adjust_account_id
        print(f"   Account ID: {adjust_account_id}")
    
    headers = {
        "Authorization": f"Bearer {ADJUST_API_TOKEN}"
    }
    
    endpoint = "https://automate.adjust.com/reports-service/csv_report"
    
    response = requests.get(endpoint, headers=headers, params=params)
    
    if response.status_code == 200:
        print("‚úÖ Donn√©es r√©cup√©r√©es avec succ√®s")
        df = pd.read_csv(io.StringIO(response.text))
        df = df.sort_values('Day (date)')
        print(f"   {len(df)} lignes r√©cup√©r√©es")
        return df
    else:
        print(f"‚ùå Erreur API: {response.status_code}")
        print(response.text)
        raise ValueError(f"Failed to retrieve data: {response.status_code}")


# =============================================================================
# FONCTIONS DE TRANSFORMATION
# =============================================================================

def transform_data(df: pd.DataFrame, config: dict) -> pd.DataFrame:
    """
    Applique les transformations sur les donn√©es.
    
    BUGS FIXES:
    - Colonnes revenue ajout√©es √† l'exclusion du groupby
    - Filtre installs > 0 corrig√© pour tous les clients Lalalab
    - Support First Purchase et autres √©v√©nements
    - Filtre pays appliqu√© d√®s le d√©but
    """
    print("üîÑ Transformation des donn√©es...")
    
    tmp = df.copy()
    client = config["client"]
    
    # ‚úÖ FILTRE PAYS EN PREMIER (avant toute autre transformation)
    if config.get("countries"):
        countries_to_keep = config["countries"]
        if "Country" in tmp.columns:
            before_count = len(tmp)
            tmp = tmp[tmp["Country"].isin(countries_to_keep)]
            print(f"   üåç Filtr√© sur pays {', '.join(countries_to_keep)}: {before_count} ‚Üí {len(tmp)} lignes")
    
    # Filtre sur Network = Sharper
    if "Network (attribution)" in tmp.columns:
        tmp = tmp[tmp["Network (attribution)"] == "Sharper"]
        print(f"   Filtr√© sur Sharper: {len(tmp)} lignes")
    
    # Filtre date de d√©but
    if config.get("start_date"):
        tmp = tmp[pd.to_datetime(tmp["Day (date)"]) >= pd.to_datetime(config["start_date"])]
        print(f"   Filtr√© depuis {config['start_date']}: {len(tmp)} lignes")
    
    # =========================================================================
    # BUG FIX #1: Filtre installs > 0
    # Le code original excluait seulement "Lalalab" exact, pas les variantes
    # =========================================================================
    CLIENTS_SANS_FILTRE_INSTALLS = [
        "Showroomprive.com - Ventes priv√©es",
        "Lalalab",
        "Lalalab Android", 
        "Lalalab Client Report Android",
        "Lalalab Client Report ios",
        "Lalalab Client Report ios & Android",
        "Bforbank - iOS",
        "Bforbank"
    ]
    
    if client not in CLIENTS_SANS_FILTRE_INSTALLS:
        before_count = len(tmp)
        tmp = tmp[tmp["Installs"] > 0]
        print(f"   Filtre installs > 0: {before_count} ‚Üí {len(tmp)} lignes")
    else:
        print(f"   ‚ö†Ô∏è  Pas de filtre installs > 0 pour {client}")
        tmp["Impressions"] = tmp["Impressions"].fillna(0)
    
    # =========================================================================
    # CUSTOM CPI pour LALALAB
    # =========================================================================
    if config.get("custom_cpi") and len(config["custom_cpi"]) > 0:
        # Initialise CPI et Adspend
        tmp['CPI'] = 0.0
        tmp['Adspend'] = 0.0
        
        # Applique les custom CPI par pays
        for country, cpi in config["custom_cpi"].items():
            print(f"   Custom CPI {country}: {cpi}‚Ç¨")
            tmp.loc[tmp["Country"] == country, "CPI"] = cpi
            tmp.loc[tmp["Country"] == country, "Adspend"] = (
                tmp.loc[tmp["Country"] == country, "Installs"].astype(float) * cpi
            )
    
    # =========================================================================
    # BUG FIX #2: Groupby avec colonnes revenue + √©v√©nements exclues
    # =========================================================================
    if config.get("group_by_most_spending_campaign"):
        print("   Grouping by most spending campaign...")
        pass
    
    # Agr√©gation finale
    if config.get("agg_columns"):
        # Colonnes num√©riques √† sommer (SANS CPI qui sera recalcul√©)
        NUMERIC_COLS_TO_SUM = [
            "Impressions", "Clicks", "Installs", "Adspend",
            "In-app revenue", "0D All revenue total", "7D All revenue total", "30D All revenue total",
            "all_revenue_total_d0", "all_revenue_total_d7", "all_revenue_total_d30"
        ]
        
        # ‚úÖ Ajoute les colonnes d'√©v√©nements (ex: First Purchase)
        # Note : Les √©v√©nements n'ont PAS de suffixes _d0/_d7/_d30 dans l'API
        event_columns = [c for c in tmp.columns if any(
            ev in c.lower() for ev in ['first_purchase', 'first purchase', 'purchase_events']
        )]
        NUMERIC_COLS_TO_SUM.extend(event_columns)
        
        # Colonnes d'agr√©gation pr√©sentes dans le DataFrame
        agg_cols = [c for c in config["agg_columns"] if c in tmp.columns]
        
        # Colonnes num√©riques pr√©sentes
        numeric_cols = list(set([c for c in NUMERIC_COLS_TO_SUM if c in tmp.columns]))
        
        print(f"   Agr√©gation sur: {agg_cols}")
        print(f"   Somme de: {numeric_cols}")
        
        # Groupby et somme
        tmp = tmp.groupby(agg_cols, as_index=False)[numeric_cols].sum()
        
        # ‚úÖ Recalcule CPI apr√®s l'agr√©gation
        if "Adspend" in tmp.columns and "Installs" in tmp.columns:
            tmp["CPI"] = tmp.apply(
                lambda row: row["Adspend"] / row["Installs"] if row["Installs"] > 0 else 0, 
                axis=1
            )
        
        print(f"   Apr√®s agr√©gation: {len(tmp)} lignes")
    
    # =========================================================================
    # REGROUPEMENT LIGNES INSTALLS=0 POUR LALALAB
    # =========================================================================
    if "Lalalab" in client:
        if "Installs" in tmp.columns:
            print(f"   Avant regroupement installs=0: {len(tmp)} lignes")
            
            # S√©pare les lignes avec et sans installs
            tmp_with_installs = tmp[tmp["Installs"] > 0].copy()
            tmp_zero_installs = tmp[tmp["Installs"] == 0].copy()
            
            if len(tmp_zero_installs) > 0:
                # Pour les lignes installs=0, on regroupe par jour
                groupby_cols = ["App", "Month (date)", "Week (date)", "Day (date)", 
                               "Network (attribution)", "Country"]
                groupby_cols = [c for c in groupby_cols if c in tmp_zero_installs.columns]
                
                # Colonnes num√©riques √† sommer (SANS CPI qui sera recalcul√©)
                numeric_cols = [c for c in tmp_zero_installs.columns 
                               if c in ["Impressions", "Clicks", "Installs", "Adspend",
                                       "In-app revenue", "0D All revenue total", 
                                       "7D All revenue total", "30D All revenue total"] or 
                                  'first_purchase' in c.lower() or 'first purchase' in c.lower()]
                
                # Regroupe les installs=0 par jour
                tmp_zero_grouped = tmp_zero_installs.groupby(groupby_cols, as_index=False)[numeric_cols].sum()
                
                # ‚úÖ Recalcule CPI apr√®s le regroupement
                if "Adspend" in tmp_zero_grouped.columns and "Installs" in tmp_zero_grouped.columns:
                    tmp_zero_grouped["CPI"] = 0  # CPI = 0 pour les lignes installs=0
                
                # Ajoute les colonnes manquantes avec valeur "other"
                tmp_zero_grouped["Campaign (attribution)"] = "other"
                tmp_zero_grouped["Adgroup (attribution)"] = "other"
                tmp_zero_grouped["Creative (attribution)"] = "other"
                
                # Recombine
                tmp = pd.concat([tmp_with_installs, tmp_zero_grouped], ignore_index=True)
                tmp = tmp.sort_values("Day (date)")
                
                print(f"   Apr√®s regroupement installs=0: {len(tmp)} lignes")
        
        # R√©ordonnancement des colonnes pour Lalalab
        lalalab_columns = [
            "App",
            "Month (date)",
            "Week (date)",
            "Day (date)",
            "Network (attribution)",
            "Country",
            "Campaign (attribution)",
            "Adgroup (attribution)",
            "Creative (attribution)",
            "Adspend",
            "Installs",
            "Impressions",
            "Clicks",
            "In-app revenue",
            "0D All revenue total",
            "7D All revenue total",
            "30D All revenue total",
            "CPI"
        ]
        
        # ‚úÖ Ajoute First Purchase si pr√©sent
        first_purchase_cols = [c for c in tmp.columns if 'first_purchase' in c.lower() or 'first purchase' in c.lower()]
        if first_purchase_cols:
            lalalab_columns.extend(first_purchase_cols)
        
        # Garde uniquement les colonnes qui existent
        existing_cols = [col for col in lalalab_columns if col in tmp.columns]
        tmp = tmp[existing_cols]
        print(f"   Colonnes Lalalab r√©ordonn√©es: {len(existing_cols)} colonnes")
    
    return tmp


# =============================================================================
# FONCTION PUSH GOOGLE SHEETS
# =============================================================================

def push_to_gsheet(df: pd.DataFrame, config: dict, gc: gspread.Client) -> str:
    """
    Push les donn√©es vers Google Sheets.
    """
    print(f"üì§ Push vers Google Sheets...")
    
    sheet_id = config["sheet_id"]
    sheet_name = config["sheet_name"]
    
    try:
        wks = gc.open_by_key(sheet_id)
        sheet = wks.worksheet(sheet_name)
        
        # Clear et push toutes les donn√©es
        sheet.clear()
        set_with_dataframe(sheet, df)
        
        url = f"https://docs.google.com/spreadsheets/d/{sheet_id}"
        print(f"‚úÖ Push r√©ussi: {url}")
        return url
        
    except Exception as e:
        print(f"‚ùå Erreur push: {e}")
        raise


def update_revenues_only(
    df_new: pd.DataFrame, 
    config: dict, 
    gc: gspread.Client,
    rolling_days: int = 30
) -> str:
    """
    Met √† jour UNIQUEMENT les colonnes revenues sur les N derniers jours.
    Garde toutes les autres donn√©es existantes intactes.
    
    Args:
        df_new: DataFrame avec les nouvelles donn√©es des N derniers jours
        config: Configuration du client
        gc: Client gspread authentifi√©
        rolling_days: Nombre de jours √† mettre √† jour (d√©faut: 30)
    
    Returns:
        URL du sheet
    """
    print(f"üì§ Mise √† jour partielle revenues (derniers {rolling_days} jours)...")
    
    sheet_id = config["sheet_id"]
    sheet_name = config["sheet_name"]
    
    try:
        wks = gc.open_by_key(sheet_id)
        sheet = wks.worksheet(sheet_name)
        
        # 1. Lire les donn√©es existantes du Google Sheet
        print("   üì• Lecture des donn√©es existantes...")
        df_existing = pd.DataFrame(sheet.get_all_records())
        
        if df_existing.empty:
            # Si le sheet est vide, push complet
            print("   ‚ö†Ô∏è  Sheet vide, push complet √† la place")
            return push_to_gsheet(df_new, config, gc)
        
        print(f"   üìä Donn√©es existantes: {len(df_existing)} lignes")
        
        # 2. Convertir les dates en datetime
        df_existing['Day (date)'] = pd.to_datetime(df_existing['Day (date)'])
        df_new['Day (date)'] = pd.to_datetime(df_new['Day (date)'])
        
        # 3. Calculer la date limite (aujourd'hui - rolling_days)
        from datetime import date, timedelta
        cutoff_date = pd.to_datetime(date.today() - timedelta(days=rolling_days))
        print(f"   üìÖ Mise √† jour des revenues depuis: {cutoff_date.strftime('%Y-%m-%d')}")
        
        # 4. Colonnes revenues √† mettre √† jour
        revenue_cols = ['0D All revenue total', '7D All revenue total', '30D All revenue total']
        # V√©rifier quelles colonnes revenues existent
        revenue_cols_to_update = [col for col in revenue_cols if col in df_existing.columns and col in df_new.columns]
        
        if not revenue_cols_to_update:
            print("   ‚ö†Ô∏è  Aucune colonne revenue trouv√©e, push complet")
            return push_to_gsheet(df_new, config, gc)
        
        print(f"   üí∞ Colonnes √† mettre √† jour: {', '.join(revenue_cols_to_update)}")
        
        # 5. Cr√©er les cl√©s de jointure (toutes les dimensions sauf Day)
        join_keys = [
            'App', 'Month (date)', 'Week (date)', 'Day (date)', 
            'Network (attribution)', 'Country',
            'Campaign (attribution)', 'Adgroup (attribution)', 'Creative (attribution)'
        ]
        # Garder uniquement les cl√©s qui existent dans les deux DataFrames
        join_keys = [k for k in join_keys if k in df_existing.columns and k in df_new.columns]
        
        # 6. S√©parer les donn√©es existantes : anciennes (> rolling_days) vs r√©centes (‚â§ rolling_days)
        df_old = df_existing[df_existing['Day (date)'] < cutoff_date].copy()
        df_recent_existing = df_existing[df_existing['Day (date)'] >= cutoff_date].copy()
        
        print(f"   üìä Donn√©es anciennes conserv√©es: {len(df_old)} lignes")
        print(f"   üìä Donn√©es r√©centes √† mettre √† jour: {len(df_recent_existing)} lignes")
        print(f"   üìä Nouvelles donn√©es: {len(df_new)} lignes")
        
        # 7. Pour les donn√©es r√©centes : remplacer les revenues par les nouvelles valeurs
        # Strat√©gie : On garde df_recent_existing et on met √† jour seulement les colonnes revenues
        
        # Cr√©er un identifiant unique pour chaque ligne
        for df_temp in [df_recent_existing, df_new]:
            df_temp['_merge_key'] = df_temp[join_keys].astype(str).agg('||'.join, axis=1)
        
        # Cr√©er un dict des nouvelles revenues
        revenue_dict = {}
        for _, row in df_new.iterrows():
            key = row['_merge_key']
            revenue_dict[key] = {col: row[col] for col in revenue_cols_to_update}
        
        # Mettre √† jour les revenues dans df_recent_existing
        updated_count = 0
        for idx, row in df_recent_existing.iterrows():
            key = row['_merge_key']
            if key in revenue_dict:
                for col in revenue_cols_to_update:
                    df_recent_existing.at[idx, col] = revenue_dict[key][col]
                updated_count += 1
        
        print(f"   ‚úÖ Revenues mises √† jour: {updated_count} lignes")
        
        # 8. Ajouter les nouvelles lignes qui n'existaient pas
        new_keys = set(df_new['_merge_key']) - set(df_recent_existing['_merge_key'])
        df_truly_new = df_new[df_new['_merge_key'].isin(new_keys)].copy()
        
        # ‚úÖ CORRECTION CRITIQUE : Retirer Ad spend et CPI des nouvelles lignes
        # pour ne PAS √©craser les valeurs existantes ou manuelles
        if len(df_truly_new) > 0:
            # Colonnes √† garder : dimensions + revenues + √©v√©nements (PAS Ad spend/CPI)
            cols_to_keep = []
            for col in df_truly_new.columns:
                # Garder les dimensions (join_keys)
                if col in join_keys:
                    cols_to_keep.append(col)
                # Garder les revenues
                elif col in revenue_cols_to_update:
                    cols_to_keep.append(col)
                # Garder First Purchase
                elif 'first' in col.lower() or 'purchase' in col.lower():
                    cols_to_keep.append(col)
                # Garder Installs, Clicks, Impressions
                elif col in ['Installs', 'Clicks', 'Impressions', 'In-app revenue']:
                    cols_to_keep.append(col)
                # EXCLURE Ad spend et CPI
                elif col not in ['Ad spend', 'CPI', '_merge_key']:
                    cols_to_keep.append(col)
            
            df_truly_new = df_truly_new[cols_to_keep]
            
            # Ajouter Ad spend et CPI √† 0 pour les nouvelles lignes
            df_truly_new['Ad spend'] = 0
            df_truly_new['CPI'] = 0
            
            print(f"   ‚ûï Nouvelles lignes ajout√©es: {len(df_truly_new)} (Ad spend/CPI = 0)")
        
        # 9. Recombiner tout
        # Supprimer la colonne _merge_key avant de combiner
        for df_temp in [df_old, df_recent_existing, df_truly_new]:
            if '_merge_key' in df_temp.columns:
                df_temp.drop('_merge_key', axis=1, inplace=True)
        
        df_final = pd.concat([df_old, df_recent_existing, df_truly_new], ignore_index=True)
        df_final = df_final.sort_values('Day (date)')
        
        print(f"   üìä Total final: {len(df_final)} lignes")
        
        # 10. Push le r√©sultat final
        sheet.clear()
        set_with_dataframe(sheet, df_final)
        
        url = f"https://docs.google.com/spreadsheets/d/{sheet_id}"
        print(f"‚úÖ Mise √† jour revenues r√©ussie: {url}")
        return url
        
    except Exception as e:
        print(f"‚ùå Erreur mise √† jour revenues: {e}")
        import traceback
        traceback.print_exc()
        raise


# =============================================================================
# FONCTION PRINCIPALE
# =============================================================================

def run_pipeline(config: dict, begin_date: str = None, end_date: str = None):
    """Ex√©cute le pipeline complet pour un client."""
    print("=" * 60)
    print(f"üöÄ PIPELINE: {config['client']}")
    print("=" * 60)
    
    # Dates par d√©faut
    if end_date is None:
        end_date = datetime.now().strftime("%Y-%m-%d")
    if begin_date is None:
        begin_date = datetime.now().replace(day=1).strftime("%Y-%m-%d")
    
    print(f"üìÖ P√©riode: {begin_date} ‚Üí {end_date}")
    
    # 1. Pull Adjust
    include_revenue = "Lalalab" in config["client"]
    
    # Dimensions sp√©cifiques pour LALALAB
    if "Lalalab" in config["client"]:
        dimensions = "app,month,week,day,country,network,campaign,creative,adgroup"
    else:
        dimensions = "day,country,network,campaign,creative,adgroup"
    
    df = pull_from_adjust(
        app_token=config["app_token"],
        begin_date=begin_date,
        end_date=end_date,
        adjust_account_id=config.get("adjust_account_id"),
        dimensions=dimensions,
        include_revenue=include_revenue
    )
    
    # 2. Transform
    df = transform_data(df, config)
    
    # 3. Affiche un aper√ßu
    print("\nüìä Aper√ßu des donn√©es:")
    print(df.head(10).to_string())
    
    # Affiche les totaux revenue
    revenue_cols = [c for c in df.columns if 'revenue' in c.lower() or 'Revenue' in c]
    if revenue_cols:
        print("\nüí∞ Totaux Revenue:")
        for col in revenue_cols:
            print(f"   {col}: {df[col].sum():,.2f}‚Ç¨")
    
    # 4. Push to GSheet
    gc = get_gspread_client()
    if gc:
        push_to_gsheet(df, config, gc)
    
    # 5. Export CSV local
    output_file = f"output_{config['client'].replace(' ', '_')}_{end_date}.csv"
    df.to_csv(output_file, index=False)
    print(f"\nüíæ Export local: {output_file}")
    
    return df


# =============================================================================
# MAIN
# =============================================================================

if __name__ == "__main__":
    from datetime import date, timedelta
    today = date.today()
    begin_date = today.replace(day=1).strftime("%Y-%m-%d")
    end_date = (today - timedelta(days=1)).strftime("%Y-%m-%d")
    
    CLIENT_TO_RUN = "LALALAB_IOS"
    
    if CLIENT_TO_RUN == "BFORBANK":
        config = BFORBANK_CONFIG
    elif CLIENT_TO_RUN == "LALALAB_IOS":
        config = LALALAB_IOS_CONFIG
    else:
        raise ValueError(f"Client inconnu: {CLIENT_TO_RUN}")
    
    df = run_pipeline(
        config=config,
        begin_date=begin_date,
        end_date=end_date
    )